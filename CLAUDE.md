# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

This is an ML experiment template for DSM-5 depression detection using the **ReDSM5 dataset** - a corpus of 1,484 Reddit posts with sentence-level clinical annotations for major depressive episode symptoms. The project combines PyTorch, Transformers, MLflow, and Optuna for reproducible ML research.

## Key Architecture Concepts

### DSM-5 Symptom Mapping

The codebase uses a critical mapping between DSM-5 criterion IDs and symptom names:

```python
CRITERION_TO_SYMPTOM = {
    'A.1': 'DEPRESSED_MOOD',
    'A.2': 'ANHEDONIA',
    'A.3': 'APPETITE_CHANGE',
    'A.4': 'SLEEP_ISSUES',
    'A.5': 'PSYCHOMOTOR',
    'A.6': 'FATIGUE',
    'A.7': 'WORTHLESSNESS',
    'A.8': 'COGNITIVE_ISSUES',
    'A.9': 'SUICIDAL_THOUGHTS',
    'A.10': 'SPECIAL_CASE'  # non-DSM-5 clinical/positive discriminations
}
```

This mapping is **essential** for all groundtruth generation scripts and should be maintained consistently across the codebase.

### Data Structure

#### Source Data (`data/redsm5/`)
- `redsm5_posts.csv` - Full post texts (1,484 posts)
  - Columns: `post_id`, `text`
- `redsm5_annotations.csv` - Sentence-level expert annotations (2,081 rows)
  - Columns: `post_id`, `sentence_id`, `sentence_text`, `DSM5_symptom`, `status`, `explanation`
  - `status=1`: positive evidence for symptom
  - `status=0`: explicit negative (clinician-annotated absence)

#### DSM-5 Criteria (`data/DSM5/`)
- `MDD_Criteira.json` - 10 DSM-5 criteria definitions
  - Note: File contains typo "Criteira" (not "Criteria")
  - Structure: `{"diagnosis": "...", "criteria": [{"id": "A.1", "text": "..."}, ...]}`

#### Generated Groundtruth (`data/groundtruth/`)
Three task-specific groundtruth files generated by scripts:

1. **`criteria_matching_groundtruth.csv`** (14,840 samples)
   - Task: Post-level NLI classification
   - Schema: `post_id, post, DSM5_symptom, groundtruth`
   - All posts × all criteria combinations

2. **`evidence_sentence_groundtruth.csv`** (298,490 samples)
   - Task: Sentence-level binary classification
   - Schema: `post_id, post, sentence_id, sentence, evidence_sentence_id, evidence_sentence, criterion, groundtruth`
   - Only annotated posts, all sentences × all criteria
   - Includes evidence sentence ID and text for each (post, criterion) pair

3. **`evidence_squad_groundtruth.csv`** (1,339 samples)
   - Task: Span extraction (SQuAD-style)
   - Schema: `post_id, post, evidence_sentence_id, evidence_sentence, criterion, start_idx, end_idx`
   - Character-level positions of evidence sentences
   - Includes evidence sentence ID and text from annotations

### Sentence Splitting

All groundtruth generation scripts use **consistent sentence splitting**:
```python
# Split on: . ! ?
pattern = r'([.!?]+\s+)'
sentences = re.split(pattern, text)
```

Sentence IDs are **0-indexed**. This splitting may differ slightly from the original annotations, causing ~40 skipped samples in SQuAD generation.

## Development Commands

### Setup
```bash
python -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate
pip install -e '.[dev]'
```

### Linting & Formatting
```bash
ruff check src tests
black src tests
```

### Testing
```bash
pytest
```

### Groundtruth Generation
```bash
# Generate all three groundtruth files
python3 scripts/generate_criteria_groundtruth.py
python3 scripts/generate_evidence_sentence_groundtruth.py
python3 scripts/generate_evidence_squad.py
```

Each script:
- Reads from `data/redsm5/` and `data/DSM5/`
- Outputs to `data/groundtruth/`
- Is idempotent (safe to re-run)
- Uses the `CRITERION_TO_SYMPTOM` mapping

## MLflow Integration

Configure MLflow for experiment tracking:

```python
from Project.utils import configure_mlflow, enable_autologging, mlflow_run

# MLflow tracking always writes to ./mlruns
configure_mlflow(tracking_uri="file:./mlruns", experiment="demo")
enable_autologging()

with mlflow_run("experiment_name", tags={"stage": "dev"}, params={"lr": 1e-4}):
    # training loop
    pass
```

Runs are logged to `mlruns/` directory by default.

## Optuna Storage

Optuna-based HPO runs must persist to the repo-level `optuna.db` file:

```python
import optuna

STORAGE_URI = "sqlite:///optuna.db"
study = optuna.create_study(
    study_name="demo",
    storage=STORAGE_URI,
    load_if_exists=True,
    direction="maximize",
)
```

## Important Notes

### File Naming
- The DSM-5 criteria file is named `MDD_Criteira.json` (typo in original dataset)
- When referencing this file, always use the exact filename

### Annotation Schema
- `status=1` in annotations means **positive evidence** for symptom
- `status=0` means **explicit negative** (not absence of annotation)
- Unlabeled (post, criterion) pairs are considered negative in groundtruth generation

### Model Architecture
- Base template uses `transformers.AutoModel` with a linear classification head
- Models expect `input_ids` and `attention_mask` inputs
- Classification head lives in `src/Project/models/model.py`

### Directory Structure
- `src/Project/` - Main package namespace (no nested SubProject module)
  - `models/` - Model definitions
  - `utils/` - Logging, seeding, MLflow helpers
  - `data/` - Dataset utilities
  - `engine/` - Training and evaluation loops
- `scripts/` - Standalone data processing scripts
- `configs/` - Configuration files
- `outputs/` - Suggested location for model artifacts
- `mlruns/` - Always-used MLflow tracking directory
- `optuna.db` - SQLite storage for Optuna HPO runs

## Citation

When using this dataset, cite:
```bibtex
@misc{bao2025redsm5,
  title        = {ReDSM5: A Reddit Dataset for DSM-5 Depression Detection},
  author       = {Eliseo Bao and Anxo Pérez and Javier Parapar},
  year         = {2025},
  eprint       = {2508.03399},
  archivePrefix= {arXiv},
  primaryClass = {cs.CL},
  note         = {Accepted at CIKM 2025}
}
```
