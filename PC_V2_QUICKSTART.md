# PC v2 Training Pipeline - Quick Start Guide

## What This Is

A **fixed** Post-Criterion (PC) cross-encoder training pipeline that:
- Implements proper 5-fold cross-validation (not the fake CV from old `train_ce_pc.py`)
- Uses conservative hyperparameters to prevent model collapse
- Applies per-class temperature calibration
- Generates true Out-of-Fold (OOF) predictions

## Prerequisites

Verify data files exist:
```bash
python scripts/verify_pc_v2_data.py
```

Expected output:
```
✓ All verification checks passed!
Ready to run: python scripts/train_pc_v2.py
```

## Training

### Option 1: Use Default Config

```bash
python scripts/train_pc_v2.py
```

This uses `configs/pc_v2.yaml` with:
- LR: 2e-5 (conservative)
- Epochs: 3
- Batch size: 16
- Max length: 512
- Output: `outputs/runs/real_dev_pc_v2/`

### Option 2: Custom Config

Create your own config (e.g., `configs/pc_v2_custom.yaml`):
```yaml
exp: my_experiment
seed: 42

model:
  name: BAAI/bge-reranker-v2-m3
  max_length: 512

train:
  lr: 2.0e-5
  epochs: 3
  batch_size: 16

output_dir: outputs/runs/my_experiment
```

Then run:
```bash
python scripts/train_pc_v2.py --config-name=pc_v2_custom
```

## Training Time

- **Per fold**: ~20-30 minutes on GPU (A100)
- **Total (5 folds)**: ~2-3 hours
- **Disk space**: ~5GB for checkpoints

## Outputs

After training completes, you'll have:

```
outputs/runs/real_dev_pc_v2/
├── pc_oof.jsonl              # OOF predictions (all 13,356 samples)
├── fold_temperatures.json    # Temperature scaling results
├── fold_0/
│   └── best_model/           # PyTorch model checkpoint
├── fold_1/best_model/
├── fold_2/best_model/
├── fold_3/best_model/
└── fold_4/best_model/
```

## Evaluation

Compute metrics on OOF predictions:

```bash
python scripts/eval_pc_v2.py \
  --oof_path outputs/runs/real_dev_pc_v2/pc_oof.jsonl \
  --output_path outputs/runs/real_dev_pc_v2/metrics.json
```

This generates `metrics.json` with:
- Per-criterion AUPRC, F1, Precision, Recall
- Macro-averaged metrics
- Optimal thresholds per criterion

## Expected Results

**Target Metrics** (based on proper CV):
- Macro-F1 (optimal): **>0.15** (acceptable), **>0.25** (good)
- Macro-AUPRC: **>0.40**
- No model collapse (diverse predictions, not all-0 or all-1)

**Class Imbalance**:
- Positive: ~1,293 (9.7%)
- Negative: ~12,063 (90.3%)

This is expected for depression detection (most post-criterion pairs are negative).

## Troubleshooting

### GPU Memory Issues

If you get CUDA OOM errors:

1. Reduce batch size in config:
   ```yaml
   train:
     batch_size: 8  # Down from 16
     gradient_accumulation: 8  # Up from 4
   ```

2. Or use CPU (slow):
   ```yaml
   train:
     bf16: false
   ```

### Model Collapse Detection

Check if predictions are all the same:
```bash
python -c "
import json
probs = [json.loads(l)['prob_cal'] for l in open('outputs/runs/real_dev_pc_v2/pc_oof.jsonl')]
print(f'Min prob: {min(probs):.4f}')
print(f'Max prob: {max(probs):.4f}')
print(f'Mean prob: {sum(probs)/len(probs):.4f}')
"
```

Healthy output:
```
Min prob: 0.0234
Max prob: 0.9123
Mean prob: 0.2345
```

Collapsed model (BAD):
```
Min prob: 0.9999
Max prob: 0.9999
Mean prob: 0.9999
```

If collapsed, try:
1. Reduce learning rate: `lr: 1.0e-5`
2. Reduce epochs: `epochs: 2`

## Comparison to Old Pipeline

| Aspect | Old (train_ce_pc.py) | New (train_pc_v2.py) |
|--------|----------------------|----------------------|
| CV Strategy | Fake (split inside loop) | True (split before loop) |
| LR | 1e-3 (too high) | 2e-5 (conservative) |
| Calibration | None | Per-class temperature |
| Dev Set | No | Yes (10% of train) |
| Model Collapse | Frequent | Prevented |
| OOF Predictions | Fake | True |

## File Structure

```
scripts/
├── train_pc_v2.py           # Main training script (577 lines)
├── eval_pc_v2.py            # Evaluation script (245 lines)
└── verify_pc_v2_data.py     # Data verification (quick check)

configs/
└── pc_v2.yaml               # Default config (48 lines)

outputs/runs/real_dev_pc_v2/
├── pc_oof.jsonl             # Generated by training
├── fold_temperatures.json   # Generated by training
├── metrics.json             # Generated by evaluation
└── fold_{0-4}/best_model/   # Model checkpoints
```

## Next Steps After Training

1. **Evaluate**: Run `eval_pc_v2.py` to get metrics
2. **Check metrics**: Ensure Macro-F1 > 0.15
3. **Analyze**: Look at per-criterion AUPRC to find weak criteria
4. **Integrate**: If good, use OOF predictions in downstream pipeline
5. **Tune** (optional): Use Optuna for hyperparameter optimization

## Key Design Decisions

1. **Conservative LR (2e-5)**: Prevents model saturation and collapse
2. **Max Length 512**: Captures full post context (posts are long)
3. **Stratified Split**: Ensures balanced symptom patterns across folds
4. **Per-Class Temperature**: Accounts for per-criterion calibration differences
5. **Dev Set**: Separate from train/test for proper calibration

## References

- Full documentation: `PC_V2_IMPLEMENTATION.md`
- Evidence pipeline (reference): `src/Project/evidence/train.py`
- Temperature scaling: `src/Project/calib/temperature.py`
- CV utilities: `src/Project/utils/cv_utils.py`

## Questions?

Check `PC_V2_IMPLEMENTATION.md` for detailed architecture and troubleshooting.
